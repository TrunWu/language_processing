{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Qian\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\Qian\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "#np.random.seed(13)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "import re\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "from sklearn.neural_network import MLPClassifier as cnn\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ù§\n",
      "üëç\n",
      "üòÑ\n",
      "üòÅ\n",
      "üò°\n"
     ]
    }
   ],
   "source": [
    "path = '../../dataset/Tweets-airline-sentiment.csv'   # ../ = upper directory\n",
    "\n",
    "print (u'\\U00002764')\n",
    "print (u'\\U0001f44d')\n",
    "print (u'\\U0001f604')\n",
    "print (u'\\U0001f601')\n",
    "print (u'\\U0001f621')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['why are your first fares in may over three times more than other carriers when all seats are available to select???',\n",
       " 'i love this graphic. http://t.co/utgrrwaaa',\n",
       " 'i love the hipster innovation. you are a feel good brand.',\n",
       " 'will you be making bos&gt;las non stop permanently anytime soon?',\n",
       " 'you guys messed up my seating.. i reserved seating with my friends and you guys gave my seat away ... üò° i want free internet',\n",
       " \"status match program.  i applied and it's been three weeks.  called and emailed with no response.\",\n",
       " \"what happened  ur vegan food options?! at least say on ur site so i know i won't be able  eat anything for next  hrs #fail\",\n",
       " \"do you miss me? don't worry we'll be together very soon.\",\n",
       " \"amazing to me that we can't get any cold air from the vents. #vx #noair #worstflightever #roasted #sfotobos\",\n",
       " 'lax to ewr - middle seat on a red eye. such a noob maneuver. #sendambien #andchexmix',\n",
       " \"hi! i just bked a cool birthday trip with you, but i can't add my elevate no. cause i entered my middle name during flight booking problems üò¢\",\n",
       " 'are the hours of operation for the club at sfo that are posted online current?',\n",
       " 'help, left expensive headphones on flight  iad to lax today. seat a. no one answering l&amp;f number at lax!',\n",
       " 'awaiting my return phone call, just would prefer to use your online self-service option :(',\n",
       " 'this is great news!  america could start flights to hawaii by end of year http://t.co/rpzyfe via ',\n",
       " 'nice rt : vibe with the moodlight from takeoff to touchdown. #moodlitmonday #sciencebehindtheexperience http://t.co/younxtqp',\n",
       " 'moodlighting is the only way to fly! best experience ever! cool and calming. üíú‚úà #moodlitmonday',\n",
       " 'done and done! best airline around, hands down!',\n",
       " 'when can i book my flight to hawaii??',\n",
       " 'your chat support is not working on your site: http://t.co/vhpgtdwpk',\n",
       " 'view of downtown los angeles, the hollywood sign, and beyond that rain in the mountains! http://t.co/dwnfibtr',\n",
       " \"hey, first time flyer next week - excited! but i'm having a hard time getting my flights added to my elevate account. help?\",\n",
       " 'plz help me win my bid upgrade for my flight / lax---&gt;sea!!!  üç∑üëçüí∫‚úàÔ∏è',\n",
       " \"i have an unused ticket but moved to a new city where you don't fly. how can i fly with you before it expires? #travelhelp\",\n",
       " 'are flights leaving dallas for seattle on time feb ?',\n",
       " \"i'm #elevategold for a good reason: you rock!!\",\n",
       " 'dream http://t.co/oadrfaoq http://t.co/lwwdackhx',\n",
       " 'wow this just blew my mind',\n",
       " 'after last night #tribute #soundofmusic #oscars ! i think agree',\n",
       " 'all were entertaining']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(path)\n",
    "text = data['text']\n",
    "label = data['airline_sentiment']\n",
    "label_tags = label.unique()\n",
    "\n",
    "#replace text label with one-hot-labels\n",
    "#new_label= []\n",
    "#for l in label:\n",
    "#    if l == label_tags[0]:\n",
    "#        new_label.append(np.array([0,0,1]))\n",
    "#    elif l == label_tags[1]:\n",
    "#        new_label.append(np.array([0,1,0]))\n",
    "#    else:\n",
    "#        new_label.append(np.array([1,0,0]))\n",
    "# above is one-hot-labels, represent labels in matrics, but can not be divided by stratifield kfold\n",
    "\n",
    "new_text = []\n",
    "for line in text:\n",
    "    line = re.sub('[0-9]','', line)\n",
    "    #line = re.sub('^@\\w+ *','', line)  # without [], ^ means match from the start\n",
    "    line = line.lower()\n",
    "    new_text.append(re.sub('@\\w+ *','', line))    #clean text and get rid of company name\n",
    "    \n",
    "new_text = new_text  # testing purpose\n",
    "new_label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['why are your first fares in may over three times more than other carriers when all seats are available to select???',\n",
       " 'i love this graphic. http://t.co/utgrrwaaa',\n",
       " 'i love the hipster innovation. you are a feel good brand.',\n",
       " 'will you be making bos&gt;las non stop permanently anytime soon?',\n",
       " 'you guys messed up my seating.. i reserved seating with my friends and you guys gave my seat away ... üò° i want free internet',\n",
       " \"status match program.  i applied and it's been three weeks.  called and emailed with no response.\",\n",
       " \"what happened  ur vegan food options?! at least say on ur site so i know i won't be able  eat anything for next  hrs #fail\",\n",
       " \"do you miss me? don't worry we'll be together very soon.\",\n",
       " \"amazing to me that we can't get any cold air from the vents. #vx #noair #worstflightever #roasted #sfotobos\",\n",
       " 'lax to ewr - middle seat on a red eye. such a noob maneuver. #sendambien #andchexmix']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(new_text)\n",
    "corpus = tokenizer.texts_to_sequences(new_text)  #Transforms each text in texts in a sequence of integers.\n",
    "#Only top \"num_words\" most frequent words will be taken into account.\n",
    "#Only words known by the tokenizer will be taken into account.\n",
    "# help(Tokenizer.texts_to_sequences)\n",
    "\n",
    "#corpus = new_text  # Qian added this line\n",
    "\n",
    "nb_samples = sum(len(s) for s in corpus)    # each s is a word(integer), this is just counting total words in corpus\n",
    "\n",
    "# The training phase is by means of the fit_on_texts method and you can see the word index using the word_index property\n",
    "# http://www.orbifold.net/default/2017/01/10/embedding-and-tokenizer-in-keras/\n",
    "V = len(tokenizer.word_index) + 1    \n",
    "# help(Tokenizer)\n",
    "\n",
    "dim = 100   # for output dimension\n",
    "window_size = 3   # this is create window for CBOW, so it takes order into consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#help(Tokenizer)\n",
    "#tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus   # for each line in twitter, it generate vectors or some kind number for each word\n",
    "# help(Tokenizer)\n",
    "# tokenizer.word_index\n",
    "# help(sequence.pad_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(corpus, window_size, V):\n",
    "    maxlen = window_size*2  # left 2 + right 2 so max 4 words\n",
    "    for words in corpus:\n",
    "        L = len(words)    # how many words are in each line in the corpus. 'words' here means 'line'\n",
    "        #print(words,L) \n",
    "        for index, word in enumerate(words):\n",
    "            contexts = []\n",
    "            labels   = []            \n",
    "            s = index - window_size   # start\n",
    "            e = index + window_size + 1   # end\n",
    "            # the window size works in a way: it only looks at the left (window size) words and the right (window size) words.\n",
    "            # here is looks at the left 2 and the right 2 words of a selected index word.\n",
    "            \n",
    "            contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "            # above: it looks at all the words from start to end in the selected range, and without look at the index word \n",
    "            \n",
    "            labels.append(word)\n",
    "            \n",
    "            x = sequence.pad_sequences(contexts, maxlen=maxlen) # pad sequence to the same length. add 0 to short sentences\n",
    "            # all sentences has to be the same length otherwise it won't work. \n",
    "              \n",
    "            y = np_utils.to_categorical(labels, V)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Sequential()  # create a sequence of actions below\n",
    "cbow.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow.add(Dense(V, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "print('start')\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='adadelta')\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c082c7b43737>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcbow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'finished'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m   1104\u001b[0m         return self.model.train_on_batch(x, y,\n\u001b[0;32m   1105\u001b[0m                                          \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m                                          class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m     def test_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data(corpus, window_size, V):\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('vectors.txt' ,'w', encoding = 'utf8')\n",
    "f.write('{} {}\\n'.format(V-1, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = cbow.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    # calculate all word vectors and save them in a text file\n",
    "    str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "    f.write('{} {}\\n'.format(word, str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding:\n",
    "    \n",
    "    def __init__(self, n): #initialize empty, with a dimension size variable\n",
    "        self.dimensions = n\n",
    "        self.wordDict = {}\n",
    "        \n",
    "    def __init__(self, fileLocation): #initialize from file\n",
    "        self.wordDict = {}\n",
    "        with open(fileLocation, encoding=\"utf-8\") as f:\n",
    "            word_n, self.dimensions = [int(x) for x in f.readline().rstrip().split(\" \")]\n",
    "            for line in f:\n",
    "                inputWord = line.rstrip().split(\" \")\n",
    "                floatArr = [float(x) for x in inputWord[1:]]\n",
    "                self.wordDict[inputWord[0]] = np.array(floatArr)\n",
    "        \n",
    "    def addWord(self, word, vector): #vector must be Numpy float array\n",
    "        if len(vector) == self.dimensions and word not in self.wordDict:\n",
    "            self.wordDict[word] = vector\n",
    "        else:\n",
    "            return False #turn into a real error message\n",
    "        \n",
    "    def getWordVector(self, word):\n",
    "        if word in self.wordDict:\n",
    "            return self.wordDict[word]\n",
    "        else:\n",
    "            return False #make a real error message\n",
    "    \n",
    "    def cosine_similarity(self, v_1, v_2):\n",
    "        upper = np.dot(v_1, v_2)\n",
    "        lower = math.sqrt(np.dot(v_1,v_1)) * math.sqrt(np.dot(v_2,v_2))\n",
    "        sim = upper / lower\n",
    "        return sim\n",
    "    \n",
    "    def wordSim(self, word1, word2):\n",
    "        return self.cosine_similarity(self.wordDict[word1],self.wordDict[word2])\n",
    "    \n",
    "    #subclass\n",
    "    class OrderedListTuple:\n",
    "        def __init__(self, max_size):\n",
    "            self.content = []\n",
    "            self.max_size = max_size\n",
    "\n",
    "        def get (self, LIST, index):\n",
    "            return LIST[index]\n",
    "    \n",
    "        def get_value(self, el):\n",
    "            return el[1]\n",
    "\n",
    "        def find_pos (self, element):\n",
    "            index = 0\n",
    "            while (index <= len(self.content)-1) and self.get_value(self.get(self.content, index)) > self.get_value(element):\n",
    "                index += 1\n",
    "            return index\n",
    "\n",
    "        def insert_element (self, element):\n",
    "            pos = self.find_pos (element)\n",
    "            self.content.insert (pos, element)\n",
    "            if len(self.content) > self.max_size:\n",
    "                self.content.pop()\n",
    "                \n",
    "    def mostSimilar(self, word, listSize=30):\n",
    "        outputList = self.OrderedListTuple(listSize)\n",
    "        v1 = self.wordDict[word]\n",
    "        for w in self.wordDict:\n",
    "            if w != word:\n",
    "                v2 = self.wordDict[w]\n",
    "                sim = self.cosine_similarity(v1,v2)\n",
    "                newTuple = (w,sim)\n",
    "                outputList.insert_element(newTuple)\n",
    "        return outputList.content\n",
    "    \n",
    "    def embedAlgebra(self, w1,w2,w3, n=1):\n",
    "        searchVector = self.wordDict[w1] + self.wordDict[w2] - self.wordDict[w3]\n",
    "        \n",
    "        outputList = self.OrderedListTuple(n)\n",
    "        for w in self.wordDict:\n",
    "            v = self.wordDict[w]\n",
    "            sim = self.cosine_similarity(searchVector,v)\n",
    "            newTuple = (w,sim)\n",
    "            outputList.insert_element(newTuple)\n",
    "\n",
    "        return outputList.content\n",
    "    \n",
    "embeddings = WordEmbedding(\"vectors_1000.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tweet_len = 20\n",
    "\n",
    "embed_text = []\n",
    "\n",
    "for t in new_text:\n",
    "    words = t.split()\n",
    "    embeds = []\n",
    "    \n",
    "    for w in words:\n",
    "        w = w.casefold()\n",
    "        #print(w)\n",
    "        w = w.strip(\",.:;_-@#!\")\n",
    "        #print(w)\n",
    "        if w in embeddings.wordDict:\n",
    "            embeds.append(embeddings.getWordVector(w))\n",
    "            \n",
    "    vec_embed = np.asarray(embeds)\n",
    "    \n",
    "    #print(t)\n",
    "    \n",
    "    if vec_embed.shape[0] > max_tweet_len:\n",
    "        vec_embed = vec_embed[:max_tweet_len, :]\n",
    "    else:\n",
    "        #print(vec_embed.shape)\n",
    "        temp_vec = np.zeros((max_tweet_len, 100))  # output dimention = 100\n",
    "        \n",
    "        if vec_embed.shape[0] > 0:\n",
    "            temp_vec[max_tweet_len - vec_embed.shape[0]:, :] = vec_embed[:,:]\n",
    "        vec_embed = temp_vec\n",
    "    \n",
    "    embed_text.append(vec_embed)\n",
    "\n",
    "embed_text = np.asarray(embed_text)\n",
    "\n",
    "print(embed_text.shape)   # this is 3D dimension, need to change to 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_embeds = np.reshape(embed_text, (embed_text.shape[0], -1))\n",
    "print(flat_embeds.shape)  # shape in 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = MultinomialNB()\n",
    "pc = Perceptron()\n",
    "svm = LinearSVC()\n",
    "lr = LogisticRegression()\n",
    "random_forest  = rf()\n",
    "KNN = knn(n_neighbors=5)\n",
    "CNN = cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "skf = SKF(n_splits = 5)\n",
    "X =  flat_embeds\n",
    "y = label[:100]\n",
    "for clf in [pc, svm, lr, KNN, CNN, random_forest]:\n",
    "    acc = []\n",
    "    for train_index, test_index in skf.split(X, y):       \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        acc.append(clf.score(X_test, y_test))\n",
    "    acc = np.asarray(acc)\n",
    "    print(clf, acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UniVec = CountVectorizer(max_features = 100, ngram_range = (1,1))\n",
    "uni = UniVec.fit_transform(new_text)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
