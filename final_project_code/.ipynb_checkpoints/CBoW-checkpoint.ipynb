{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "#np.random.seed(13)\n",
    "\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import gensim\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "import re\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "from sklearn.neural_network import MLPClassifier as cnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../dataset/Tweets-airline-sentiment.csv'   # ../ = upper directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path)\n",
    "text = data['text']\n",
    "label = data['airline_sentiment']\n",
    "label_tags = label.unique()\n",
    "\n",
    "\n",
    "#replace text label with one-hot-labels\n",
    "#new_label= []\n",
    "#for l in label:\n",
    "#    if l == label_tags[0]:\n",
    "#        new_label.append(np.array([0,0,1]))\n",
    "#    elif l == label_tags[1]:\n",
    "#        new_label.append(np.array([0,1,0]))\n",
    "#    else:\n",
    "#        new_label.append(np.array([1,0,0]))\n",
    "# above is one-h\n",
    "\n",
    "new_text = []\n",
    "for line in text:\n",
    "    new_text.append(re.sub('0-9^@\\\\w+ *','', line))    #clean text and get rid of company name\n",
    "    \n",
    "new_text = new_text[:500]  # testing purpose\n",
    "new_label = label[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#new_text[:10]\n",
    "#new_label[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(new_text)\n",
    "corpus = tokenizer.texts_to_sequences(new_text)  #Transforms each text in texts in a sequence of integers.\n",
    "#Only top \"num_words\" most frequent words will be taken into account.\n",
    "#Only words known by the tokenizer will be taken into account.\n",
    "# help(Tokenizer.texts_to_sequences)\n",
    "\n",
    "#corpus = new_text  # Qian added this line\n",
    "\n",
    "nb_samples = sum(len(s) for s in corpus)    # each s is a word(integer), this is just counting total words in corpus\n",
    "\n",
    "# The training phase is by means of the fit_on_texts method and you can see the word index using the word_index property\n",
    "# http://www.orbifold.net/default/2017/01/10/embedding-and-tokenizer-in-keras/\n",
    "V = len(tokenizer.word_index) + 1    \n",
    "# help(Tokenizer)\n",
    "\n",
    "dim = 100   # for output dimension\n",
    "window_size = 2   # this is create window for CBOW, so it takes order into consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#help(Tokenizer)\n",
    "#tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus   # for each line in twitter, it generate vectors or some kind number for each word\n",
    "# help(Tokenizer)\n",
    "# tokenizer.word_index\n",
    "# help(sequence.pad_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(corpus, window_size, V):\n",
    "    maxlen = window_size*2  # left 2 + right 2 so max 4 words\n",
    "    for words in corpus:\n",
    "        L = len(words)    # how many words are in each line in the corpus. 'words' here means 'line'\n",
    "        #print(words,L) \n",
    "        for index, word in enumerate(words):\n",
    "            contexts = []\n",
    "            labels   = []            \n",
    "            s = index - window_size   # start\n",
    "            e = index + window_size + 1   # end\n",
    "            # the window size works in a way: it only looks at the left (window size) words and the right (window size) words.\n",
    "            # here is looks at the left 2 and the right 2 words of a selected index word.\n",
    "            \n",
    "            contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "            # above: it looks at all the words from start to end in the selected range, and without look at the index word \n",
    "            \n",
    "            labels.append(word)\n",
    "            \n",
    "            x = sequence.pad_sequences(contexts, maxlen=maxlen) # pad sequence to the same length. add 0 to short sentences\n",
    "            # all sentences has to be the same length otherwise it won't work. \n",
    "              \n",
    "            y = np_utils.to_categorical(labels, V)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Sequential()  # create a sequence of actions below\n",
    "cbow.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow.add(Dense(V, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 53526.9822640419\n",
      "1 49940.376394331455\n",
      "2 49412.55982582271\n",
      "3 49200.223310031\n",
      "4 49019.62169538811\n",
      "5 48854.44552107155\n",
      "6 48698.0265290644\n",
      "7 48550.73634926416\n",
      "8 48417.70369774988\n",
      "9 48296.55130845634\n"
     ]
    }
   ],
   "source": [
    "for ite in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data(corpus, window_size, V):\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "    print(ite, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('vectors.txt' ,'w', encoding = 'utf8')\n",
    "f.write('{} {}\\n'.format(V-1, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = cbow.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    # calculate all word vectors and save them in a text file\n",
    "    str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "    f.write('{} {}\\n'.format(word, str_vec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors_1000.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('36', 0.6086865663528442),\n",
       " ('visa', 0.5834057331085205),\n",
       " ('turbulence', 0.5777522325515747),\n",
       " ('paying', 0.5694185495376587),\n",
       " (\"you'd\", 0.5638517141342163),\n",
       " ('gratitude', 0.563335657119751),\n",
       " ('atlantic', 0.5582131147384644),\n",
       " ('phl', 0.5564830303192139),\n",
       " ('before', 0.5534918308258057),\n",
       " (\"that's\", 0.5526996850967407)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('crew', 0.7087175846099854),\n",
       " ('little', 0.7064733505249023),\n",
       " ('america', 0.7045174837112427),\n",
       " ('using', 0.6980020403862),\n",
       " ('visa', 0.6953568458557129),\n",
       " ('poor', 0.688417911529541),\n",
       " ('together', 0.6866887211799622),\n",
       " ('turbulence', 0.6859854459762573),\n",
       " ('again', 0.6859509348869324),\n",
       " ('consumers', 0.6804114580154419)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding:\n",
    "    \n",
    "    def __init__(self, n): #initialize empty, with a dimension size variable\n",
    "        self.dimensions = n\n",
    "        self.wordDict = {}\n",
    "        \n",
    "    def __init__(self, fileLocation): #initialize from file\n",
    "        self.wordDict = {}\n",
    "        with open(fileLocation, encoding=\"utf-8\") as f:\n",
    "            word_n, self.dimensions = [int(x) for x in f.readline().rstrip().split(\" \")]\n",
    "            for line in f:\n",
    "                inputWord = line.rstrip().split(\" \")\n",
    "                floatArr = [float(x) for x in inputWord[1:]]\n",
    "                self.wordDict[inputWord[0]] = np.array(floatArr)\n",
    "        \n",
    "    def addWord(self, word, vector): #vector must be Numpy float array\n",
    "        if len(vector) == self.dimensions and word not in self.wordDict:\n",
    "            self.wordDict[word] = vector\n",
    "        else:\n",
    "            return False #turn into a real error message\n",
    "        \n",
    "    def getWordVector(self, word):\n",
    "        if word in self.wordDict:\n",
    "            return self.wordDict[word]\n",
    "        else:\n",
    "            return False #make a real error message\n",
    "    \n",
    "    def cosine_similarity(self, v_1, v_2):\n",
    "        upper = np.dot(v_1, v_2)\n",
    "        lower = math.sqrt(np.dot(v_1,v_1)) * math.sqrt(np.dot(v_2,v_2))\n",
    "        sim = upper / lower\n",
    "        return sim\n",
    "    \n",
    "    def wordSim(self, word1, word2):\n",
    "        return self.cosine_similarity(self.wordDict[word1],self.wordDict[word2])\n",
    "    \n",
    "    #subclass\n",
    "    class OrderedListTuple:\n",
    "        def __init__(self, max_size):\n",
    "            self.content = []\n",
    "            self.max_size = max_size\n",
    "\n",
    "        def get (self, LIST, index):\n",
    "            return LIST[index]\n",
    "    \n",
    "        def get_value(self, el):\n",
    "            return el[1]\n",
    "\n",
    "        def find_pos (self, element):\n",
    "            index = 0\n",
    "            while (index <= len(self.content)-1) and self.get_value(self.get(self.content, index)) > self.get_value(element):\n",
    "                index += 1\n",
    "            return index\n",
    "\n",
    "        def insert_element (self, element):\n",
    "            pos = self.find_pos (element)\n",
    "            self.content.insert (pos, element)\n",
    "            if len(self.content) > self.max_size:\n",
    "                self.content.pop()\n",
    "                \n",
    "    def mostSimilar(self, word, listSize=30):\n",
    "        outputList = self.OrderedListTuple(listSize)\n",
    "        v1 = self.wordDict[word]\n",
    "        for w in self.wordDict:\n",
    "            if w != word:\n",
    "                v2 = self.wordDict[w]\n",
    "                sim = self.cosine_similarity(v1,v2)\n",
    "                newTuple = (w,sim)\n",
    "                outputList.insert_element(newTuple)\n",
    "\n",
    "        return outputList.content\n",
    "    \n",
    "    def embedAlgebra(self, w1,w2,w3, n=1):\n",
    "        searchVector = self.wordDict[w1] + self.wordDict[w2] - self.wordDict[w3]\n",
    "        \n",
    "        outputList = self.OrderedListTuple(n)\n",
    "        for w in self.wordDict:\n",
    "            v = self.wordDict[w]\n",
    "            sim = self.cosine_similarity(searchVector,v)\n",
    "            newTuple = (w,sim)\n",
    "            outputList.insert_element(newTuple)\n",
    "\n",
    "        return outputList.content\n",
    "    \n",
    "embeddings = WordEmbedding(\"vectors_1000.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 20, 100)\n"
     ]
    }
   ],
   "source": [
    "max_tweet_len = 20\n",
    "\n",
    "embed_text = []\n",
    "\n",
    "for t in new_text:\n",
    "    words = t.split()\n",
    "    embeds = []\n",
    "    \n",
    "    for w in words:\n",
    "        w = w.casefold()\n",
    "        #print(w)\n",
    "        w = w.strip(\",.:;_-@#!\")\n",
    "        #print(w)\n",
    "        if w in embeddings.wordDict:\n",
    "            embeds.append(embeddings.getWordVector(w))\n",
    "            \n",
    "    vec_embed = np.asarray(embeds)\n",
    "    \n",
    "    #print(t)\n",
    "    \n",
    "    if vec_embed.shape[0] > max_tweet_len:\n",
    "        vec_embed = vec_embed[:max_tweet_len, :]\n",
    "    else:\n",
    "        #print(vec_embed.shape)\n",
    "        temp_vec = np.zeros((max_tweet_len, 100))  # output dimention = 100\n",
    "        \n",
    "        if vec_embed.shape[0] > 0:\n",
    "            temp_vec[max_tweet_len - vec_embed.shape[0]:, :] = vec_embed[:,:]\n",
    "        vec_embed = temp_vec\n",
    "    \n",
    "    embed_text.append(vec_embed)\n",
    "\n",
    "embed_text = np.asarray(embed_text)\n",
    "\n",
    "print(embed_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 2000)\n"
     ]
    }
   ],
   "source": [
    "flat_embeds = np.reshape(embed_text, (embed_text.shape[0], -1))\n",
    "print(flat_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = MultinomialNB()\n",
    "pc = Perceptron()\n",
    "svm = LinearSVC()\n",
    "lr = LogisticRegression()\n",
    "random_forest  = rf()\n",
    "KNN = knn(n_neighbors=3)\n",
    "CNN = cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47474747474747475\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold as SKF\n",
    "skf = SKF(n_splits = 5)\n",
    "X =  flat_embeds\n",
    "y = label[:500]\n",
    "#clf = pc\n",
    "clf = lr\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    acc = []\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc.append(clf.score(X_test, y_test))\n",
    "    acc = np.asarray(acc)\n",
    "print(acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
