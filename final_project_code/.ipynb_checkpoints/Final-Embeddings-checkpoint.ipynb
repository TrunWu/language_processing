{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "import re\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "from sklearn.neural_network import MLPClassifier as cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Object to save words and their embeddings\n",
    "class WordEmbedding:\n",
    "    \n",
    "    def __init__(self, n): #initialize empty, with a dimension size variable\n",
    "        self.dimensions = n\n",
    "        self.wordDict = {}\n",
    "        \n",
    "    def __init__(self, fileLocation): #initialize from file\n",
    "        self.wordDict = {}\n",
    "        with open(fileLocation, encoding=\"utf-8\") as f:\n",
    "            word_n, self.dimensions = [int(x) for x in f.readline().rstrip().split(\" \")]\n",
    "            for line in f:\n",
    "                inputWord = line.rstrip().split(\" \")\n",
    "                floatArr = [float(x) for x in inputWord[1:]]\n",
    "                self.wordDict[inputWord[0]] = np.array(floatArr)\n",
    "        \n",
    "    def addWord(self, word, vector): #vector must be Numpy float array\n",
    "        if len(vector) == self.dimensions and word not in self.wordDict:\n",
    "            self.wordDict[word] = vector\n",
    "        else:\n",
    "            return False #turn into a real error message\n",
    "        \n",
    "    def getWordVector(self, word):\n",
    "        if word in self.wordDict:\n",
    "            return self.wordDict[word]\n",
    "        else:\n",
    "            return False #make a real error message\n",
    "    \n",
    "    def cosine_similarity(self, v_1, v_2):\n",
    "        upper = np.dot(v_1, v_2)\n",
    "        lower = math.sqrt(np.dot(v_1,v_1)) * math.sqrt(np.dot(v_2,v_2))\n",
    "        sim = upper / lower\n",
    "        return sim\n",
    "    \n",
    "    def wordSim(self, word1, word2):\n",
    "        return self.cosine_similarity(self.wordDict[word1],self.wordDict[word2])\n",
    "    \n",
    "    #subclass\n",
    "    class OrderedListTuple:\n",
    "        def __init__(self, max_size):\n",
    "            self.content = []\n",
    "            self.max_size = max_size\n",
    "\n",
    "        def get (self, LIST, index):\n",
    "            return LIST[index]\n",
    "    \n",
    "        def get_value(self, el):\n",
    "            return el[1]\n",
    "\n",
    "        def find_pos (self, element):\n",
    "            index = 0\n",
    "            while (index <= len(self.content)-1) and self.get_value(self.get(self.content, index)) > self.get_value(element):\n",
    "                index += 1\n",
    "            return index\n",
    "\n",
    "        def insert_element (self, element):\n",
    "            pos = self.find_pos (element)\n",
    "            self.content.insert (pos, element)\n",
    "            if len(self.content) > self.max_size:\n",
    "                self.content.pop()\n",
    "                \n",
    "    def mostSimilar(self, word, listSize=30):\n",
    "        outputList = self.OrderedListTuple(listSize)\n",
    "        v1 = self.wordDict[word]\n",
    "        for w in self.wordDict:\n",
    "            if w != word:\n",
    "                v2 = self.wordDict[w]\n",
    "                sim = self.cosine_similarity(v1,v2)\n",
    "                newTuple = (w,sim)\n",
    "                outputList.insert_element(newTuple)\n",
    "\n",
    "        return outputList.content\n",
    "    \n",
    "    def embedAlgebra(self, w1,w2,w3, n=1):\n",
    "        searchVector = self.wordDict[w1] + self.wordDict[w2] - self.wordDict[w3]\n",
    "        \n",
    "        outputList = self.OrderedListTuple(n)\n",
    "        for w in self.wordDict:\n",
    "            v = self.wordDict[w]\n",
    "            sim = self.cosine_similarity(searchVector,v)\n",
    "            newTuple = (w,sim)\n",
    "            outputList.insert_element(newTuple)\n",
    "\n",
    "        return outputList.content\n",
    "    \n",
    "embeddings = WordEmbedding(\"wiki.en.vec.short\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path1 = '../dataset/Tweets-airline-sentiment.csv'\n",
    "#data_path2 = '../dataset/labeledTrainData_head12000.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = data['airline_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral' 'positive' 'negative']\n"
     ]
    }
   ],
   "source": [
    "label_tags = label.unique()\n",
    "print(label_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replace text label with one-hot-labels\n",
    "new_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for l in label:\n",
    "    if l == label_tags[0]:\n",
    "        new_label.append(0)\n",
    "    elif l == label_tags[1]:\n",
    "        new_label.append(1)\n",
    "    else:\n",
    "        new_label.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_label = np.asarray(new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get rid of '@airline_company_name\n",
    "new_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t in text:\n",
    "    new_text.append(re.sub('^@\\w+ *','', t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_text = np.asarray(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14640,), (14640,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text.shape, new_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeds Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_tweet_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640, 5, 300)\n"
     ]
    }
   ],
   "source": [
    "embed_text = []\n",
    "\n",
    "for t in new_text:\n",
    "    words = t.split()\n",
    "    embeds = []\n",
    "    \n",
    "    for w in words:\n",
    "        w = w.casefold()\n",
    "        #print(w)\n",
    "        w = w.strip(\",.:;_-@#!\")\n",
    "        #print(w)\n",
    "        if w in embeddings.wordDict:\n",
    "            embeds.append(embeddings.getWordVector(w))\n",
    "            \n",
    "    vec_embed = np.asarray(embeds)\n",
    "    \n",
    "    #print(t)\n",
    "    \n",
    "    if vec_embed.shape[0] > max_tweet_len:\n",
    "        vec_embed = vec_embed[:max_tweet_len, :]\n",
    "    else:\n",
    "        #print(vec_embed.shape)\n",
    "        temp_vec = np.zeros((max_tweet_len, 300))\n",
    "        if vec_embed.shape[0] > 0:\n",
    "            temp_vec[max_tweet_len - vec_embed.shape[0]:, :] = vec_embed[:,:]\n",
    "        vec_embed = temp_vec\n",
    "    \n",
    "    embed_text.append(vec_embed)\n",
    "\n",
    "embed_text = np.asarray(embed_text)\n",
    "\n",
    "print(embed_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640, 1500)\n"
     ]
    }
   ],
   "source": [
    "flat_embeds = np.reshape(embed_text, (embed_text.shape[0], -1))\n",
    "print(flat_embeds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NB = MultinomialNB()\n",
    "pc = Perceptron()\n",
    "svm = LinearSVC()\n",
    "lr = LogisticRegression()\n",
    "random_forest  = rf()\n",
    "KNN = knn(n_neighbors=3)\n",
    "CNN = cnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.607359637233\n",
      "0.678897582854\n",
      "0.687435602624\n"
     ]
    }
   ],
   "source": [
    "for clf in [pc, svm, lr]:\n",
    "    acc = []\n",
    "    for train_index, test_index in skf.split(flat_embeds, new_label):\n",
    "        x_train,x_test = flat_embeds[train_index], flat_embeds[test_index]\n",
    "        y_train, y_test = new_label[train_index], new_label[test_index]\n",
    "        clf.fit(x_train, y_train)\n",
    "        acc.append(clf.score(x_test, y_test))\n",
    "    acc = np.asarray(acc)\n",
    "    print(acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.630056092732\n",
      "0.680603856893\n",
      "0.669607614045\n"
     ]
    }
   ],
   "source": [
    "for clf in [KNN, CNN, random_forest]:\n",
    "    acc = []\n",
    "    for train_index, test_index in skf.split(flat_embeds, new_label):\n",
    "        x_train,x_test = flat_embeds[train_index], flat_embeds[test_index]\n",
    "        y_train, y_test = new_label[train_index], new_label[test_index]\n",
    "        clf.fit(x_train, y_train)\n",
    "        acc.append(clf.score(x_test, y_test))\n",
    "    acc = np.asarray(acc)\n",
    "    print(acc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UniVec = CountVectorizer(max_features = 500, ngram_range = (1,1))\n",
    "uni = UniVec.fit_transform(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7301962457671689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7111350376695293\n",
      "0.7618886777939342\n",
      "0.7667370754555864\n"
     ]
    }
   ],
   "source": [
    "for clf in [NB, pc, svm, lr]:\n",
    "    acc = []\n",
    "    for train_index, test_index in skf.split(uni, new_label):\n",
    "        x_train,x_test = uni[train_index], uni[test_index]\n",
    "        y_train, y_test = new_label[train_index], new_label[test_index]\n",
    "        clf.fit(x_train, y_train)\n",
    "        acc.append(clf.score(x_test, y_test))\n",
    "    acc = np.asarray(acc)\n",
    "    print(acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49044845012281224\n",
      "0.7411243931760308\n",
      "0.7334056494059733\n"
     ]
    }
   ],
   "source": [
    "for clf in [KNN, CNN, random_forest]:\n",
    "    acc = []\n",
    "    for train_index, test_index in skf.split(uni, new_label):\n",
    "        x_train,x_test = uni[train_index], uni[test_index]\n",
    "        y_train, y_test = new_label[train_index], new_label[test_index]\n",
    "        clf.fit(x_train, y_train)\n",
    "        acc.append(clf.score(x_test, y_test))\n",
    "    acc = np.asarray(acc)\n",
    "    print(acc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BiVec = CountVectorizer(max_features = 500, ngram_range = (2,2))\n",
    "Bi = BiVec.fit_transform(new_text)\n",
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7301962457671689\n",
      "0.7111350376695293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7618886777939342\n",
      "0.7667370754555864\n",
      "0.49044845012281224\n",
      "0.7298525695649101\n"
     ]
    }
   ],
   "source": [
    "for clf in [NB, pc, svm, lr, KNN, random_forest]:\n",
    "    acc = []\n",
    "    for train_index, test_index in skf.split(uni, new_label):\n",
    "        x_train,x_test = uni[train_index], uni[test_index]\n",
    "        y_train, y_test = new_label[train_index], new_label[test_index]\n",
    "        clf.fit(x_train, y_train)\n",
    "        acc.append(clf.score(x_test, y_test))\n",
    "    acc = np.asarray(acc)\n",
    "    print(acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.743923727176688\n"
     ]
    }
   ],
   "source": [
    "for clf in [CNN]:\n",
    "    acc = []\n",
    "    for train_index, test_index in skf.split(uni, new_label):\n",
    "        x_train,x_test = uni[train_index], uni[test_index]\n",
    "        y_train, y_test = new_label[train_index], new_label[test_index]\n",
    "        clf.fit(x_train, y_train)\n",
    "        acc.append(clf.score(x_test, y_test))\n",
    "    acc = np.asarray(acc)\n",
    "    print(acc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uni&Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MixVec = CountVectorizer(max_features = 500, ngram_range = (1,2))\n",
    "Mix = BiVec.fit_transform(new_text)\n",
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7301962457671689\n",
      "0.7111350376695293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7618886777939342\n",
      "0.7667370754555864\n",
      "0.49044845012281224\n",
      "0.7326511330773402\n"
     ]
    }
   ],
   "source": [
    "for clf in [NB, pc, svm, lr, KNN, random_forest]:\n",
    "    acc = []\n",
    "    for train_index, test_index in skf.split(uni, new_label):\n",
    "        x_train,x_test = uni[train_index], uni[test_index]\n",
    "        y_train, y_test = new_label[train_index], new_label[test_index]\n",
    "        clf.fit(x_train, y_train)\n",
    "        acc.append(clf.score(x_test, y_test))\n",
    "    acc = np.asarray(acc)\n",
    "    print(acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7448794744777792\n"
     ]
    }
   ],
   "source": [
    "for clf in [CNN]:\n",
    "    acc = []\n",
    "    for train_index, test_index in skf.split(uni, new_label):\n",
    "        x_train,x_test = uni[train_index], uni[test_index]\n",
    "        y_train, y_test = new_label[train_index], new_label[test_index]\n",
    "        clf.fit(x_train, y_train)\n",
    "        acc.append(clf.score(x_test, y_test))\n",
    "    acc = np.asarray(acc)\n",
    "    print(acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
