{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build CNN to do training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cnn import mini_XCEPTION\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dataset import read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../dataset/Tweets-airline-sentiment.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path, feature = 'Unigram', max_feature_num = 500):\n",
    "#feature: the text feature, could be 'Unigram, Bigram, Trigram or Mixing Unigram with Bigram'\n",
    "\tdata = pd.read_csv(data_path)\n",
    "\ttext = data['text']\n",
    "\tlabel = data['airline_sentiment']\n",
    "\tlabel_tags = label.unique()\n",
    "\t#replace text label with one-hot-labels\n",
    "\tnew_label= []\n",
    "\tfor l in label:\n",
    "\t\tif l == label_tags[0]:\n",
    "\t\t\tnew_label.append(np.array([0,0,1]))\n",
    "\t\telif l == label_tags[1]:\n",
    "\t\t\tnew_label.append(np.array([0,1,0]))\n",
    "\t\telse:\n",
    "\t\t\tnew_label.append(np.array([1,0,0]))\n",
    "\t#get rid of '@airline_company_name\n",
    "\tnew_text = []\n",
    "\tfor t in text:\n",
    "\t\tnew_text.append(re.sub('^@\\\\w+ *','', t))\n",
    "\tif feature == 'Unigram':\n",
    "\t\tVec = CountVectorizer(max_features = max_feature_num, ngram_range=(1,1))\n",
    "\t\tout = Vec.fit_transform(new_text)\n",
    "\telif feature == 'Bigram':\n",
    "\t\tVec = CountVectorizer(max_features = max_feature_num, ngram_range=(2,2))\n",
    "\t\tout = Vec.fit_transform(new_text)\n",
    "\telif feature == 'Trigram':\n",
    "\t\tVec = CountVectorizer(max_features= max_feature_num, ngram_range=(3,3))\n",
    "\t\tout = Vec.fit_transform(new_text)\n",
    "\telse:\n",
    "\t# mix bigram and unigram\n",
    "\t\tVec = CountVectorizer(max_features = max_feature_num, ngram_range = (1,2))\n",
    "\t\tout = Vec.fit_transform(new_text)\n",
    "\tnew_label = np.asarray(new_label)\n",
    "\treturn out, new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, label = read_data(data_path = data_path, feature='Unigram', max_feature_num=500)\n",
    "#text, label = np.asarray(text), np.asarray(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = np.asarray(text.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "newt = []\n",
    "height, width = 10,50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in text:\n",
    "    newt.append(t.reshape((height, width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = np.asarray(newt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 50)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 1000\n",
    "input_shape = (text.shape[1],text.shape[2],1)\n",
    "verbose = 1\n",
    "number_classes = 3\n",
    "patience = 50\n",
    "# data generator\n",
    "data_generator = ImageDataGenerator(\n",
    "                        featurewise_center=False,\n",
    "                        featurewise_std_normalization=False,\n",
    "                        rotation_range=10,\n",
    "                        width_shift_range=0.1,\n",
    "                        height_shift_range=0.1,\n",
    "                        zoom_range=.1,\n",
    "                        horizontal_flip=True,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mini_XCEPTION(input_shape=input_shape, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 10, 50, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 48, 8)     72          input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 48, 8)     32          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 8, 48, 8)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 6, 46, 8)     576         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 6, 46, 8)     32          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 6, 46, 8)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCo (None, 6, 46, 16)    200         activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 6, 46, 16)    64          separable_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 6, 46, 16)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_10 (SeparableC (None, 6, 46, 16)    400         activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 6, 46, 16)    64          separable_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 3, 23, 16)    128         activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 3, 23, 16)    0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 3, 23, 16)    64          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 3, 23, 16)    0           max_pooling2d_5[0][0]            \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_11 (SeparableC (None, 3, 23, 32)    656         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 3, 23, 32)    128         separable_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 3, 23, 32)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_12 (SeparableC (None, 3, 23, 32)    1312        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 3, 23, 32)    128         separable_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 2, 12, 32)    512         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 2, 12, 32)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 2, 12, 32)    128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 2, 12, 32)    0           max_pooling2d_6[0][0]            \n",
      "                                                                 batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_13 (SeparableC (None, 2, 12, 64)    2336        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 2, 12, 64)    256         separable_conv2d_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 2, 12, 64)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_14 (SeparableC (None, 2, 12, 64)    4672        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 2, 12, 64)    256         separable_conv2d_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 1, 6, 64)     2048        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 1, 6, 64)     0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 1, 6, 64)     256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 1, 6, 64)     0           max_pooling2d_7[0][0]            \n",
      "                                                                 batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_15 (SeparableC (None, 1, 6, 128)    8768        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 1, 6, 128)    512         separable_conv2d_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 1, 6, 128)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_16 (SeparableC (None, 1, 6, 128)    17536       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 1, 6, 128)    512         separable_conv2d_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 1, 3, 128)    8192        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 1, 3, 128)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 1, 3, 128)    512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 1, 3, 128)    0           max_pooling2d_8[0][0]            \n",
      "                                                                 batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 1, 3, 3)      3459        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 3)            0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Activation)        (None, 3)            0           global_average_pooling2d_2[0][0] \n",
      "==================================================================================================\n",
      "Total params: 53,811\n",
      "Trainable params: 52,339\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "dataset_name = 'USAirline'\n",
    "log_file_path = dataset_name + '_sentiment_training.log'\n",
    "csv_logger = CSVLogger(log_file_path, append=False)\n",
    "early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1,patience=int(patience/4), verbose=1)\n",
    "trained_models_path = dataset_name + '_mini_XCEPTION'\n",
    "model_names = trained_models_path + '.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,save_best_only=True)\n",
    "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(text, label, train_test_split = 0.3,validation_train_split = 0.2):\n",
    "    #number of each dataset\n",
    "    arr_num = label.shape[0]\n",
    "    test_num = int(arr_num*train_test_split)\n",
    "    train_num = int((arr_num-test_num)*(1-validation_train_split))\n",
    "    val_num = arr_num - test_num-train_num\n",
    "    #new permulated dataset\n",
    "    permutation_arr = np.random.permutation(arr_num)\n",
    "    #new_text = np.expand_dims(text,-1)\n",
    "    new_text, new_label = text[permutation_arr], label[permutation_arr]\n",
    "    X_train, X_val, X_test = new_text[:train_num], new_text[train_num: (train_num+val_num)], new_text[(train_num+val_num):]\n",
    "    y_train, y_val, y_test = new_label[:train_num], new_label[train_num: (train_num+val_num)], new_label[(train_num+val_num):]\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(text, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = np.expand_dims(X_train,-1),np.expand_dims(X_val,-1),np.expand_dims(X_test,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8198, 10, 50, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = (X_train,y_train)\n",
    "val_data = (X_val, y_val)\n",
    "test_data = (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8198, 10, 50, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.image.NumpyArrayIterator at 0x7f8dbaf5f5f8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_generator.flow(X_train,y_train,batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "257/256 [==============================] - 34s 131ms/step - loss: 0.9656 - acc: 0.6370 - val_loss: 0.9914 - val_acc: 0.6337\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.99137, saving model to USAirline_mini_XCEPTION.01-0.63.hdf5\n",
      "Epoch 2/1000\n",
      "257/256 [==============================] - 29s 111ms/step - loss: 0.9250 - acc: 0.6364 - val_loss: 0.9559 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.99137 to 0.95593, saving model to USAirline_mini_XCEPTION.02-0.64.hdf5\n",
      "Epoch 3/1000\n",
      "257/256 [==============================] - 29s 112ms/step - loss: 0.9007 - acc: 0.6431 - val_loss: 0.8719 - val_acc: 0.6459\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.95593 to 0.87193, saving model to USAirline_mini_XCEPTION.03-0.65.hdf5\n",
      "Epoch 4/1000\n",
      "257/256 [==============================] - 31s 119ms/step - loss: 0.8768 - acc: 0.6492 - val_loss: 0.8816 - val_acc: 0.6449\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.87193\n",
      "Epoch 5/1000\n",
      "257/256 [==============================] - 30s 118ms/step - loss: 0.8688 - acc: 0.6485 - val_loss: 0.8820 - val_acc: 0.6259\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.87193\n",
      "Epoch 6/1000\n",
      "257/256 [==============================] - 29s 113ms/step - loss: 0.8617 - acc: 0.6445 - val_loss: 0.9049 - val_acc: 0.6415\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.87193\n",
      "Epoch 7/1000\n",
      "257/256 [==============================] - 29s 113ms/step - loss: 0.8600 - acc: 0.6434 - val_loss: 0.8559 - val_acc: 0.6473\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.87193 to 0.85593, saving model to USAirline_mini_XCEPTION.07-0.65.hdf5\n",
      "Epoch 8/1000\n",
      "257/256 [==============================] - 29s 113ms/step - loss: 0.8493 - acc: 0.6486 - val_loss: 0.8253 - val_acc: 0.6532\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.85593 to 0.82530, saving model to USAirline_mini_XCEPTION.08-0.65.hdf5\n",
      "Epoch 9/1000\n",
      "257/256 [==============================] - 29s 113ms/step - loss: 0.8433 - acc: 0.6509 - val_loss: 0.8369 - val_acc: 0.6454\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.82530\n",
      "Epoch 10/1000\n",
      "257/256 [==============================] - 29s 115ms/step - loss: 0.8439 - acc: 0.6484 - val_loss: 0.8417 - val_acc: 0.6463\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.82530\n",
      "Epoch 11/1000\n",
      "257/256 [==============================] - 30s 116ms/step - loss: 0.8375 - acc: 0.6484 - val_loss: 0.8206 - val_acc: 0.6473\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.82530 to 0.82063, saving model to USAirline_mini_XCEPTION.11-0.65.hdf5\n",
      "Epoch 12/1000\n",
      "257/256 [==============================] - 30s 116ms/step - loss: 0.8341 - acc: 0.6489 - val_loss: 0.8380 - val_acc: 0.6429\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.82063\n",
      "Epoch 13/1000\n",
      "257/256 [==============================] - 29s 112ms/step - loss: 0.8333 - acc: 0.6533 - val_loss: 0.8289 - val_acc: 0.6551\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.82063\n",
      "Epoch 14/1000\n",
      "257/256 [==============================] - 30s 116ms/step - loss: 0.8353 - acc: 0.6463 - val_loss: 0.8269 - val_acc: 0.6502\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.82063\n",
      "Epoch 15/1000\n",
      "257/256 [==============================] - 30s 117ms/step - loss: 0.8321 - acc: 0.6489 - val_loss: 0.8236 - val_acc: 0.6439\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.82063\n",
      "Epoch 16/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.8301 - acc: 0.6515 - val_loss: 0.8430 - val_acc: 0.6498\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.82063\n",
      "Epoch 17/1000\n",
      "257/256 [==============================] - 30s 115ms/step - loss: 0.8332 - acc: 0.6476 - val_loss: 0.8311 - val_acc: 0.6424\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.82063\n",
      "Epoch 18/1000\n",
      "257/256 [==============================] - 29s 113ms/step - loss: 0.8302 - acc: 0.6466 - val_loss: 0.8246 - val_acc: 0.6498\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.82063\n",
      "Epoch 19/1000\n",
      "257/256 [==============================] - 31s 120ms/step - loss: 0.8296 - acc: 0.6525 - val_loss: 0.8135 - val_acc: 0.6502\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.82063 to 0.81346, saving model to USAirline_mini_XCEPTION.19-0.65.hdf5\n",
      "Epoch 20/1000\n",
      "257/256 [==============================] - 32s 126ms/step - loss: 0.8255 - acc: 0.6541 - val_loss: 0.8287 - val_acc: 0.6468\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.81346\n",
      "Epoch 21/1000\n",
      "257/256 [==============================] - 32s 124ms/step - loss: 0.8251 - acc: 0.6500 - val_loss: 0.8156 - val_acc: 0.6561\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.81346\n",
      "Epoch 22/1000\n",
      "257/256 [==============================] - 32s 126ms/step - loss: 0.8239 - acc: 0.6544 - val_loss: 0.8612 - val_acc: 0.6434\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.81346\n",
      "Epoch 23/1000\n",
      "257/256 [==============================] - 32s 125ms/step - loss: 0.8222 - acc: 0.6529 - val_loss: 0.8227 - val_acc: 0.6498\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.81346\n",
      "Epoch 24/1000\n",
      "257/256 [==============================] - 31s 120ms/step - loss: 0.8275 - acc: 0.6545 - val_loss: 0.8146 - val_acc: 0.6595\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.81346\n",
      "Epoch 25/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.8227 - acc: 0.6516 - val_loss: 0.8276 - val_acc: 0.6473\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.81346\n",
      "Epoch 26/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.8244 - acc: 0.6526 - val_loss: 0.8281 - val_acc: 0.6449\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.81346\n",
      "Epoch 27/1000\n",
      "257/256 [==============================] - 30s 117ms/step - loss: 0.8215 - acc: 0.6555 - val_loss: 0.8161 - val_acc: 0.6600\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.81346\n",
      "Epoch 28/1000\n",
      "257/256 [==============================] - 29s 111ms/step - loss: 0.8197 - acc: 0.6547 - val_loss: 0.8189 - val_acc: 0.6605\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.81346\n",
      "Epoch 29/1000\n",
      "257/256 [==============================] - 31s 121ms/step - loss: 0.8237 - acc: 0.6557 - val_loss: 0.8210 - val_acc: 0.6473\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.81346\n",
      "Epoch 30/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.8246 - acc: 0.6568 - val_loss: 0.8174 - val_acc: 0.6527\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.81346\n",
      "Epoch 31/1000\n",
      "257/256 [==============================] - 31s 120ms/step - loss: 0.8254 - acc: 0.6521 - val_loss: 0.8110 - val_acc: 0.6659\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.81346 to 0.81096, saving model to USAirline_mini_XCEPTION.31-0.67.hdf5\n",
      "Epoch 32/1000\n",
      "257/256 [==============================] - 29s 115ms/step - loss: 0.8224 - acc: 0.6507 - val_loss: 0.8087 - val_acc: 0.6483\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.81096 to 0.80865, saving model to USAirline_mini_XCEPTION.32-0.65.hdf5\n",
      "Epoch 33/1000\n",
      "257/256 [==============================] - 29s 115ms/step - loss: 0.8185 - acc: 0.6545 - val_loss: 0.8264 - val_acc: 0.6556\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.80865\n",
      "Epoch 34/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.8184 - acc: 0.6563 - val_loss: 0.8263 - val_acc: 0.6517\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.80865\n",
      "Epoch 35/1000\n",
      "257/256 [==============================] - 30s 115ms/step - loss: 0.8159 - acc: 0.6574 - val_loss: 0.8709 - val_acc: 0.6449\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.80865\n",
      "Epoch 36/1000\n",
      "257/256 [==============================] - 31s 119ms/step - loss: 0.8174 - acc: 0.6554 - val_loss: 0.8251 - val_acc: 0.6488\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.80865\n",
      "Epoch 37/1000\n",
      "257/256 [==============================] - 30s 116ms/step - loss: 0.8162 - acc: 0.6574 - val_loss: 0.8244 - val_acc: 0.6576\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.80865\n",
      "Epoch 38/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.8175 - acc: 0.6597 - val_loss: 0.8100 - val_acc: 0.6615\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.80865\n",
      "Epoch 39/1000\n",
      "257/256 [==============================] - 30s 115ms/step - loss: 0.8191 - acc: 0.6566 - val_loss: 0.8098 - val_acc: 0.6605\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.80865\n",
      "Epoch 40/1000\n",
      "257/256 [==============================] - 29s 115ms/step - loss: 0.8151 - acc: 0.6581 - val_loss: 0.8126 - val_acc: 0.6590\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.80865\n",
      "Epoch 41/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/256 [==============================] - 29s 111ms/step - loss: 0.8159 - acc: 0.6510 - val_loss: 0.8214 - val_acc: 0.6585\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.80865\n",
      "Epoch 42/1000\n",
      "257/256 [==============================] - 29s 111ms/step - loss: 0.8139 - acc: 0.6544 - val_loss: 0.8013 - val_acc: 0.6605\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.80865 to 0.80132, saving model to USAirline_mini_XCEPTION.42-0.66.hdf5\n",
      "Epoch 43/1000\n",
      "257/256 [==============================] - 29s 112ms/step - loss: 0.8177 - acc: 0.6569 - val_loss: 0.8129 - val_acc: 0.6654\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.80132\n",
      "Epoch 44/1000\n",
      "257/256 [==============================] - 29s 112ms/step - loss: 0.8109 - acc: 0.6622 - val_loss: 0.8290 - val_acc: 0.6507\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.80132\n",
      "Epoch 45/1000\n",
      "257/256 [==============================] - 29s 112ms/step - loss: 0.8116 - acc: 0.6615 - val_loss: 0.8208 - val_acc: 0.6527\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.80132\n",
      "Epoch 46/1000\n",
      "257/256 [==============================] - 29s 112ms/step - loss: 0.8120 - acc: 0.6599 - val_loss: 0.8185 - val_acc: 0.6532\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.80132\n",
      "Epoch 47/1000\n",
      "257/256 [==============================] - 29s 113ms/step - loss: 0.8089 - acc: 0.6603 - val_loss: 0.8458 - val_acc: 0.6473\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.80132\n",
      "Epoch 48/1000\n",
      "257/256 [==============================] - 29s 113ms/step - loss: 0.8162 - acc: 0.6558 - val_loss: 0.8242 - val_acc: 0.6449\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.80132\n",
      "Epoch 49/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.8146 - acc: 0.6551 - val_loss: 0.8343 - val_acc: 0.6551\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.80132\n",
      "Epoch 50/1000\n",
      "257/256 [==============================] - 29s 115ms/step - loss: 0.8166 - acc: 0.6575 - val_loss: 0.8511 - val_acc: 0.6473\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.80132\n",
      "Epoch 51/1000\n",
      "257/256 [==============================] - 29s 113ms/step - loss: 0.8156 - acc: 0.6582 - val_loss: 0.8284 - val_acc: 0.6629\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.80132\n",
      "Epoch 52/1000\n",
      "257/256 [==============================] - 29s 113ms/step - loss: 0.8121 - acc: 0.6570 - val_loss: 0.8146 - val_acc: 0.6571\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.80132\n",
      "Epoch 53/1000\n",
      "257/256 [==============================] - 29s 113ms/step - loss: 0.8162 - acc: 0.6541 - val_loss: 0.8268 - val_acc: 0.6551\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.80132\n",
      "Epoch 54/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.8143 - acc: 0.6571 - val_loss: 0.8151 - val_acc: 0.6454\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.80132\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 55/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.8057 - acc: 0.6631 - val_loss: 0.8045 - val_acc: 0.6473\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.80132\n",
      "Epoch 56/1000\n",
      "257/256 [==============================] - 29s 115ms/step - loss: 0.7979 - acc: 0.6587 - val_loss: 0.8029 - val_acc: 0.6483\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.80132\n",
      "Epoch 57/1000\n",
      "257/256 [==============================] - 30s 115ms/step - loss: 0.7990 - acc: 0.6616 - val_loss: 0.8016 - val_acc: 0.6571\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.80132\n",
      "Epoch 58/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.8019 - acc: 0.6601 - val_loss: 0.8028 - val_acc: 0.6546\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.80132\n",
      "Epoch 59/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.7993 - acc: 0.6652 - val_loss: 0.8021 - val_acc: 0.6629\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.80132\n",
      "Epoch 60/1000\n",
      "257/256 [==============================] - 30s 115ms/step - loss: 0.7965 - acc: 0.6587 - val_loss: 0.8035 - val_acc: 0.6654\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.80132\n",
      "Epoch 61/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.7962 - acc: 0.6626 - val_loss: 0.8036 - val_acc: 0.6683\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.80132\n",
      "Epoch 62/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.7970 - acc: 0.6629 - val_loss: 0.8014 - val_acc: 0.6678\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.80132\n",
      "Epoch 63/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.7963 - acc: 0.6600 - val_loss: 0.8028 - val_acc: 0.6693\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.80132\n",
      "Epoch 64/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.7947 - acc: 0.6679 - val_loss: 0.8032 - val_acc: 0.6678\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.80132\n",
      "Epoch 65/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.7972 - acc: 0.6622 - val_loss: 0.8034 - val_acc: 0.6659\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.80132\n",
      "Epoch 66/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.7971 - acc: 0.6625 - val_loss: 0.8018 - val_acc: 0.6654\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.80132\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 67/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.7917 - acc: 0.6663 - val_loss: 0.8019 - val_acc: 0.6644\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.80132\n",
      "Epoch 68/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.7911 - acc: 0.6645 - val_loss: 0.8024 - val_acc: 0.6644\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.80132\n",
      "Epoch 69/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.7909 - acc: 0.6618 - val_loss: 0.8024 - val_acc: 0.6644\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.80132\n",
      "Epoch 70/1000\n",
      "257/256 [==============================] - 30s 116ms/step - loss: 0.7929 - acc: 0.6644 - val_loss: 0.8024 - val_acc: 0.6654\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.80132\n",
      "Epoch 71/1000\n",
      "257/256 [==============================] - 30s 116ms/step - loss: 0.7945 - acc: 0.6646 - val_loss: 0.8022 - val_acc: 0.6649\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.80132\n",
      "Epoch 72/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.7910 - acc: 0.6671 - val_loss: 0.8018 - val_acc: 0.6659\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.80132\n",
      "Epoch 73/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.7892 - acc: 0.6646 - val_loss: 0.8020 - val_acc: 0.6649\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.80132\n",
      "Epoch 74/1000\n",
      "257/256 [==============================] - 29s 115ms/step - loss: 0.7952 - acc: 0.6661 - val_loss: 0.8016 - val_acc: 0.6673\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.80132\n",
      "Epoch 75/1000\n",
      "257/256 [==============================] - 30s 115ms/step - loss: 0.7926 - acc: 0.6690 - val_loss: 0.8019 - val_acc: 0.6654\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.80132\n",
      "Epoch 76/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.7936 - acc: 0.6632 - val_loss: 0.8016 - val_acc: 0.6668\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.80132\n",
      "Epoch 77/1000\n",
      "257/256 [==============================] - 30s 115ms/step - loss: 0.7889 - acc: 0.6701 - val_loss: 0.8018 - val_acc: 0.6668\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.80132\n",
      "Epoch 78/1000\n",
      "257/256 [==============================] - 29s 114ms/step - loss: 0.7984 - acc: 0.6621 - val_loss: 0.8022 - val_acc: 0.6639\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.80132\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 79/1000\n",
      "257/256 [==============================] - 30s 115ms/step - loss: 0.7885 - acc: 0.6662 - val_loss: 0.8019 - val_acc: 0.6673\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.80132\n",
      "Epoch 80/1000\n",
      "257/256 [==============================] - 30s 115ms/step - loss: 0.7937 - acc: 0.6644 - val_loss: 0.8017 - val_acc: 0.6654\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.80132\n",
      "Epoch 81/1000\n",
      "257/256 [==============================] - 30s 115ms/step - loss: 0.7945 - acc: 0.6626 - val_loss: 0.8020 - val_acc: 0.6663\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.80132\n",
      "Epoch 82/1000\n",
      "257/256 [==============================] - 30s 116ms/step - loss: 0.7929 - acc: 0.6645 - val_loss: 0.8017 - val_acc: 0.6678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00082: val_loss did not improve from 0.80132\n",
      "Epoch 83/1000\n",
      "257/256 [==============================] - 28s 107ms/step - loss: 0.7950 - acc: 0.6626 - val_loss: 0.8018 - val_acc: 0.6678\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.80132\n",
      "Epoch 84/1000\n",
      "257/256 [==============================] - 28s 108ms/step - loss: 0.7931 - acc: 0.6628 - val_loss: 0.8018 - val_acc: 0.6659\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.80132\n",
      "Epoch 85/1000\n",
      "257/256 [==============================] - 28s 108ms/step - loss: 0.7986 - acc: 0.6617 - val_loss: 0.8019 - val_acc: 0.6678\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.80132\n",
      "Epoch 86/1000\n",
      "257/256 [==============================] - 28s 108ms/step - loss: 0.7980 - acc: 0.6640 - val_loss: 0.8019 - val_acc: 0.6663\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.80132\n",
      "Epoch 87/1000\n",
      "257/256 [==============================] - 29s 113ms/step - loss: 0.7945 - acc: 0.6629 - val_loss: 0.8019 - val_acc: 0.6654\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.80132\n",
      "Epoch 88/1000\n",
      "257/256 [==============================] - 29s 113ms/step - loss: 0.7958 - acc: 0.6631 - val_loss: 0.8017 - val_acc: 0.6663\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.80132\n",
      "Epoch 89/1000\n",
      "257/256 [==============================] - 29s 113ms/step - loss: 0.7939 - acc: 0.6629 - val_loss: 0.8016 - val_acc: 0.6659\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.80132\n",
      "Epoch 90/1000\n",
      "257/256 [==============================] - 29s 113ms/step - loss: 0.7925 - acc: 0.6699 - val_loss: 0.8019 - val_acc: 0.6668\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.80132\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 91/1000\n",
      "257/256 [==============================] - 30s 117ms/step - loss: 0.7897 - acc: 0.6658 - val_loss: 0.8019 - val_acc: 0.6668\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.80132\n",
      "Epoch 92/1000\n",
      "257/256 [==============================] - 29s 113ms/step - loss: 0.7938 - acc: 0.6619 - val_loss: 0.8018 - val_acc: 0.6668\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.80132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8dbaf3e438>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#traing\n",
    "model.fit_generator(data_generator.flow(X_train,y_train,batch_size = 32),\n",
    "                        steps_per_epoch=len(X_train) / batch_size,\n",
    "                        epochs=num_epochs, verbose=1, callbacks=callbacks,\n",
    "                        validation_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './USAirline_mini_XCEPTION.31-0.67.hdf5'\n",
    "classifier = load_model(emotion_model_path, compile=False)\n",
    "# getting input model shapes for inference\n",
    "target_size = classifier.input_shape[1:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gray_image in X_test:\n",
    "    #gray_image = np.squeeze(gray_image)\n",
    "    #gray_image = preprocess_input(gray_image, True)\n",
    "    gray_image = np.expand_dims(gray_image, 0)\n",
    "    #gray_image = np.expand_dims(gray_image, -1)\n",
    "    label = np.argmax(classifier.predict(gray_image))\n",
    "    pred_label.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6568761384335154\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for pred,true_label in zip(pred_label, np.argmax(y_test,axis=1)):\n",
    "    if pred == true_label:\n",
    "        acc += 1 \n",
    "    else:\n",
    "        continue\n",
    "print(acc/len(y_test))\n",
    "        \n",
    "                           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
