{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "import re\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "from sklearn.neural_network import MLPClassifier as cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path1 = '../dataset/Tweets-airline-sentiment.csv'\n",
    "#data_path2 = '../dataset/labeledTrainData_head12000.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data['airline_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral' 'positive' 'negative']\n"
     ]
    }
   ],
   "source": [
    "label_tags = label.unique()\n",
    "print(label_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace text label with one-hot-labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label = []\n",
    "for l in label:\n",
    "    if l == label_tags[0]:\n",
    "        #new_label.append(np.array([0,0,1]))\n",
    "        new_label.append(0)\n",
    "    elif l == label_tags[1]:\n",
    "        #new_label.append(np.array([0,1,0]))\n",
    "        new_label.append(1)\n",
    "    else:\n",
    "        #new_label.append(np.array([1,0,0]))\n",
    "        new_label.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label = np.asarray(new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of '@airline_company_name\n",
    "new_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in text:\n",
    "    new_text.append(re.sub('^@\\w+ *','', t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = np.asarray(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14640,), (14640,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text.shape, new_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x, y, n_split, test_train_split = 0.2):\n",
    "# x: traning test, in array\n",
    "#y: label\n",
    "#n_split: number of groups divided from x and y\n",
    "    if x.shape[0] != y.shape[0]:\n",
    "        raise('samples and their labels\\'number mismatched')\n",
    "    else:\n",
    "        instance_num = x.shape[0]\n",
    "        train_num = int(instance_num*(1-test_train_split))\n",
    "        permulated_arr = np.random.permutation(instance_num)\n",
    "        train_arr, test_arr = permulated_arr[:train_num], permulated_arr[train_num:]\n",
    "        train_each_num = int(train_num/n_split)\n",
    "        test_each_num = int((instance_num-train_num)/n_split)\n",
    "        for i in range(n_split):\n",
    "            if i == n_split-1:\n",
    "                #train index and test index\n",
    "                yield (train_arr[i*train_each_num:], test_arr[i*test_each_num:])\n",
    "            else:\n",
    "                yield (train_arr[i*train_each_num:(i+1)*train_each_num], test_arr[i*test_each_num:(i+1)*test_each_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2342,)\n",
      "(585,)\n",
      "(2342,)\n",
      "(585,)\n",
      "(2342,)\n",
      "(585,)\n",
      "(2342,)\n",
      "(585,)\n",
      "(2344,)\n",
      "(588,)\n"
     ]
    }
   ],
   "source": [
    "for train, test in split(uni, new_label,5):\n",
    "    print(train.shape)\n",
    "    print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = MultinomialNB()\n",
    "pc = Perceptron()\n",
    "svm = LinearSVC()\n",
    "lr = LogisticRegression()\n",
    "random_forest  = rf()\n",
    "\n",
    "CNN = cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = knn(n_neighbors=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "UniVec = CountVectorizer(max_features = 500, ngram_range = (1,1))\n",
    "uni = UniVec.fit_transform(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni = np.asarray(uni.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 500)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7455674578775445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7357973167295876\n",
      "0.7697455739085189\n",
      "0.7799217947218307\n"
     ]
    }
   ],
   "source": [
    "for clf in [NB, pc, svm, lr]:\n",
    "    acc = []\n",
    "    for train_index, test_index in skf.split(uni, new_label):\n",
    "        x_train,x_test = uni[train_index], uni[test_index]\n",
    "        y_train, y_test = new_label[train_index], new_label[test_index]\n",
    "        clf.fit(x_train, y_train)\n",
    "        acc.append(clf.score(x_test, y_test))\n",
    "    acc = np.asarray(acc)\n",
    "    print(acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor clf in [ NB,pc, svm, lr]:\\n    acc = []\\n    for train_index, test_index in split(uni, new_label,5):\\n        x_train,x_test = uni[train_index], uni[test_index]\\n        y_train, y_test = new_label[train_index], new_label[test_index]\\n        clf.fit(x_train, y_train)\\n        acc.append(clf.score(x_test, y_test))\\n    acc = np.asarray(acc)\\n    print(acc.mean())\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use my split()\n",
    "'''\n",
    "for clf in [ NB,pc, svm, lr]:\n",
    "    acc = []\n",
    "    for train_index, test_index in split(uni, new_label,5):\n",
    "        x_train,x_test = uni[train_index], uni[test_index]\n",
    "        y_train, y_test = new_label[train_index], new_label[test_index]\n",
    "        clf.fit(x_train, y_train)\n",
    "        acc.append(clf.score(x_test, y_test))\n",
    "    acc = np.asarray(acc)\n",
    "    print(acc.mean())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4887411030651731\n",
      "0.7307385410044167\n"
     ]
    }
   ],
   "source": [
    "for clf in [KNN, random_forest]:\n",
    "    acc = []\n",
    "    for train_index, test_index in skf.split(uni, new_label):\n",
    "        x_train,x_test = uni[train_index], uni[test_index]\n",
    "        y_train, y_test = new_label[train_index], new_label[test_index]\n",
    "        clf.fit(x_train, y_train)\n",
    "        acc.append(clf.score(x_test, y_test))\n",
    "    acc = np.asarray(acc)\n",
    "    print(acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in [CNN]:\n",
    "    acc = []\n",
    "    for train_index, test_index in skf.split(uni, new_label):\n",
    "        x_train,x_test = uni[train_index], uni[test_index]\n",
    "        y_train, y_test = new_label[train_index], new_label[test_index]\n",
    "        clf.fit(x_train, y_train)\n",
    "        acc.append(clf.score(x_test, y_test))\n",
    "    acc = np.asarray(acc)\n",
    "    print(acc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiVec = CountVectorizer(max_features = 1000, ngram_range = (2,2))\n",
    "Bi = BiVec.fit_transform(new_text)\n",
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6807406780510499\n",
      "0.6459729581957722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6883211764972066\n",
      "0.7031416470449299\n",
      "0.5224757379084302\n",
      "0.6462472329414705\n"
     ]
    }
   ],
   "source": [
    "for clf in [NB, pc, svm, lr, KNN, random_forest]:\n",
    "    acc = []\n",
    "    for train_index, test_index in skf.split(Bi, new_label):\n",
    "        x_train,x_test = Bi[train_index], Bi[test_index]\n",
    "        y_train, y_test = new_label[train_index], new_label[test_index]\n",
    "        clf.fit(x_train, y_train)\n",
    "        acc.append(clf.score(x_test, y_test))\n",
    "    acc = np.asarray(acc)\n",
    "    print(acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6692664559871363\n"
     ]
    }
   ],
   "source": [
    "for clf in [CNN]:\n",
    "    acc = []\n",
    "    for train_index, test_index in skf.split(Bi, new_label):\n",
    "        x_train,x_test = Bi[train_index], Bi[test_index]\n",
    "        y_train, y_test = new_label[train_index], new_label[test_index]\n",
    "        clf.fit(x_train, y_train)\n",
    "        acc.append(clf.score(x_test, y_test))\n",
    "    acc = np.asarray(acc)\n",
    "    print(acc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uni&Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MixVec = CountVectorizer(max_features = 1000, ngram_range = (1,2))\n",
    "Mix = BiVec.fit_transform(new_text)\n",
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6807406780510499\n",
      "0.6459729581957722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6883211764972066\n",
      "0.7031416470449299\n",
      "0.5224757379084302\n",
      "0.649184321720502\n"
     ]
    }
   ],
   "source": [
    "for clf in [NB, pc, svm, lr, KNN, random_forest]:\n",
    "    acc = []\n",
    "    for train_index, test_index in skf.split(Mix, new_label):\n",
    "        x_train,x_test = Mix[train_index], Mix[test_index]\n",
    "        y_train, y_test = new_label[train_index], new_label[test_index]\n",
    "        clf.fit(x_train, y_train)\n",
    "        acc.append(clf.score(x_test, y_test))\n",
    "    acc = np.asarray(acc)\n",
    "    print(acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6656449069229424\n"
     ]
    }
   ],
   "source": [
    "for clf in [CNN]:\n",
    "    acc = []\n",
    "    for train_index, test_index in skf.split(Mix, new_label):\n",
    "        x_train,x_test = Mix[train_index], Mix[test_index]\n",
    "        y_train, y_test = new_label[train_index], new_label[test_index]\n",
    "        clf.fit(x_train, y_train)\n",
    "        acc.append(clf.score(x_test, y_test))\n",
    "    acc = np.asarray(acc)\n",
    "    print(acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
